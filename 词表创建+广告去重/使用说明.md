这部分对应分词器词表创建以及广告清洗的步骤。 \
由于词表迭代需要对所有数据进行多次分词，数据量非常大，处理十分耗时，这里的代码包含临时的修改与调试，仅供参考。 \
耗时操作进行了充分的并行处理，但对内存的需求较大。 \
经过分析，发现计算速度是瓶颈，可以放心使用交换分区，交换分区建议临时增加到200G。 \
推测使用pybind11重写分词器代码可以有效加速，当前分词速率约2Mtoken/s/进程。 \
词表迭代迭代时，各长度词汇分布变化如图： \
<img width="613" height="441" alt="image" src="https://github.com/user-attachments/assets/cba1930a-ab81-4c6a-8c31-ec2f5b9a4127" /> \
可以看到初始时，有的词汇降低，有的词汇词频升高，这是在充分利用最大概率路径算法的特性获得真正有效的词汇，去除无效片段。 \
<img width="607" height="435" alt="image" src="https://github.com/user-attachments/assets/7d1d4a01-ef89-491a-9625-8acdd2a54f00" /> \
<img width="613" height="447" alt="image" src="https://github.com/user-attachments/assets/eda24f8e-b30f-471d-8eb5-60de6ae6adb0" /> \
最终，词频趋于稳定，无效词汇基本都被去除。但是词表还是较大，无法使用。 \
每次去除词频最低的部分词，再重新分词统计词频，不断迭代，直到词表所见到期望大小。 \
这里提供了“最终词表”与“三个大小合适的中间词表”共参考，可以发现最终词表中词汇质量很高。 \
需要说明的是，与训练耗时（4090一周）相比，词表创建的时间消耗近似（16核CPU数天），是十分暴力但有效的算法。 \
如果机器配置足够（比如64核，256G内存），程序继续优化（分词器用pybind11、C++重写），词表创建的耗时可能控制在一天之内。
