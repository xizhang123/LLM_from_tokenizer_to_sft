{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5190fa7b-8278-4c3f-9245-9b834e4e2d49",
   "metadata": {},
   "source": [
    "# 将分词器bug修复后,继续将词表压缩到262144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7d430a-2da2-462b-b720-36e15d1d05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from collections import Counter,defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f602d6e6-0b7a-4741-bdfc-e75c0d6ca16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "候选词表大小: 338723\n",
      "CPU times: user 450 ms, sys: 20.1 ms, total: 471 ms\n",
      "Wall time: 468 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#加载词表，只使用词频特征\n",
    "pattern = re.compile(r'^[\\u4e00-\\u9fff]+$')\n",
    "words_count = defaultdict(int)\n",
    "with open('vocabs/vocab_b_338724.txt','r',encoding='utf-8') as f:\n",
    "    for word in f:\n",
    "        if word[0]!='\\t':\n",
    "            k,v = word.split('\\t')\n",
    "            v = int(v[:-1])\n",
    "        if len(k)==1 or pattern.match(k):\n",
    "            words_count[k] += v\n",
    "print('候选词表大小:',len(words_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff46134-6cb9-4348-955b-1bdf118bbe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 83.9 ms, sys: 1.9 ms, total: 85.8 ms\n",
      "Wall time: 85.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "N = 7 #最长的n-gram片段\n",
    "count_sum = [0 for _ in range(N)]\n",
    "words = [word for word in words_count]\n",
    "for word in words:\n",
    "    count_sum[len(word)-1] += words_count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6178807d-b59e-4084-8af8-07c9755f6e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤阈值： 3877 预计词表大小： 262144\n"
     ]
    }
   ],
   "source": [
    "T = 12\n",
    "vocab_init   = 5690450\n",
    "vocab_target = 262144\n",
    "gap = (vocab_init/vocab_target)**(1/T)\n",
    "remain_list = [int(vocab_init/gap**i+0.5) for i in range(1,T+1)]\n",
    "thr = 0.0\n",
    "#对所有词汇的词频进行排序，用于确定筛选阈值\n",
    "v_list = list(words_count.values())\n",
    "v_list.sort()\n",
    "#在遇到第一个可用阈值时停止\n",
    "for r in remain_list:\n",
    "    if r < len(words_count):\n",
    "        thr = v_list[-r]\n",
    "        print(\"过滤阈值：\",thr,\"预计词表大小：\",r)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42cf6470-11a1-4920-b542-d48dbbf902da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 651 ms, sys: 19 ms, total: 670 ms\n",
      "Wall time: 672 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import ahocorasick as ah\n",
    "aca= ah.Automaton()\n",
    "for word in words:\n",
    "    #构造自动机时对词频进行过滤\n",
    "    if words_count[word] > thr:\n",
    "        aca.add_word(word.encode(),(len(word.encode()),np.log(words_count[word]/count_sum[len(word)-1])))\n",
    "aca.make_automaton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e2b8ea-4a21-4574-855c-0fd7f261360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(encode_text,alpha=1.0):\n",
    "    #路径，记录起始位置和分值\n",
    "    LOT = len(encode_text)\n",
    "    BOW = 0 #表示最佳词的起始位置\n",
    "    VOW = 1 #表示最佳路径的累积值\n",
    "    VOID = 5 #表示没有记录\n",
    "    routes = [(i,VOID) for i in range(LOT)] + [(-1,0.0)]\n",
    "    tokens = []  #保存分词结果\n",
    "    #遍历所有匹配成功的词\n",
    "    # low:len_of_word\n",
    "    # vow:value_of_word\n",
    "    for eow, (low,vow) in aca.iter(encode_text):\n",
    "        #匹配词起点序号 = 匹配词终点序号 -（匹配词长度-1）\n",
    "        bow = eow - low + 1\n",
    "        #得分是负数，但负的程度越小约好，\n",
    "        #数值为起始位置的得分 + 当前词的分数\n",
    "        #起始位置无记录就往前找\n",
    "        i = 0\n",
    "        while routes[bow - 1 - i][VOW] == VOID:\n",
    "            i += 1\n",
    "        v = routes[bow - 1 -i][VOW] + vow\n",
    "        # 超过5.0直接使用确定算法\n",
    "        if alpha >= 5.0:\n",
    "            #更短的路径或第一个到达，更新\n",
    "            if v > routes[eow][VOW] and i == 0 or routes[eow][VOW] == VOID:\n",
    "                routes[eow] = bow,v #记录起始位置以及累积值\n",
    "        else:\n",
    "            # 随机算法\n",
    "            if routes[eow][VOW] == VOID:\n",
    "                base = v\n",
    "                temp = 1.0\n",
    "                denominator = 1.0\n",
    "                routes[eow] = bow,v\n",
    "            else:\n",
    "                temp = np.exp(alpha * (v - base))\n",
    "                denominator += temp\n",
    "                if np.random.rand() < temp/denominator and i == 0:\n",
    "                    routes[eow] = bow,v #记录起始位置以及累积值\n",
    "    #     print(w,vow,v)\n",
    "    # print([str(item[1])[:5] for item in routes])\n",
    "    #从后往前查找分割点\n",
    "    eow = LOT - 1\n",
    "    while encode_text:\n",
    "        bow = routes[eow][BOW] #找到最佳词的起始位置\n",
    "        tokens.append(encode_text[bow:eow+1]) #记录该词语\n",
    "        encode_text,eow = encode_text[:bow],bow - 1 #继续分上一个词\n",
    "    # tokens = tokens[::-1] #从后往前找，需要反序得到正序的分词结果注释掉加速\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54ba8bb6-7c8b-4b12-a1af-e86ff7dd7dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对多个字符串计算字节数与token数量\n",
    "def get_words_counter(i):\n",
    "    global str_lists\n",
    "    temp_counter = Counter()\n",
    "    for encode_text in str_lists[i]:\n",
    "        temp_counter.update(tokenizer(encode_text,1.0))\n",
    "    return temp_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25584e71-8c64-45b0-b070-2b170c716af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:29<00:00,  6.71s/it]\n",
      "2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:45<00:00,  7.00s/it]\n",
      "3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:39<00:00,  6.88s/it]\n",
      "4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:31<00:00,  6.76s/it]\n",
      "5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:28<00:00,  6.69s/it]\n",
      "6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:32<00:00,  6.76s/it]\n",
      "7: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:28<00:00,  6.69s/it]\n",
      "8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:32<00:00,  6.77s/it]\n",
      "9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:29<00:00,  6.72s/it]\n",
      "10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:31<00:00,  6.75s/it]\n",
      "11: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:27<00:00,  6.68s/it]\n",
      "12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:32<00:00,  6.77s/it]\n",
      "13: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:31<00:00,  6.75s/it]\n",
      "14: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:32<00:00,  6.77s/it]\n",
      "15: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:30<00:00,  6.73s/it]\n",
      "16: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:32<00:00,  6.77s/it]\n",
      "17: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:24<00:00,  6.63s/it]\n",
      "18: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:15<00:00,  6.47s/it]\n",
      "19: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:14<00:00,  6.46s/it]\n",
      "20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:16<00:00,  6.50s/it]\n",
      "21: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:19<00:00,  6.54s/it]\n",
      "22: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [06:18<00:00,  6.53s/it]\n",
      "23: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [05:07<00:00,  9.60s/it]\n"
     ]
    }
   ],
   "source": [
    "#获取数据集中的全部文件\n",
    "batch = 10000\n",
    "new_counter = Counter()\n",
    "str_list = []\n",
    "str_len = 0\n",
    "cnt = 0\n",
    "with open('high_data.txt','r',encoding='utf-8') as f:\n",
    "    for item in f:\n",
    "        str_list += [item.encode()]\n",
    "        str_len  += len(item)\n",
    "        if str_len > 1e9:\n",
    "            str_lists = [str_list[i*batch:(i+1)*batch] for i in range(len(str_list)//batch + (len(str_list)%batch != 0))]\n",
    "            str_list = []\n",
    "            str_len = 0\n",
    "            cnt += 1\n",
    "            #分词并记录词频\n",
    "            with concurrent.futures.ProcessPoolExecutor(max_workers=15) as executor:\n",
    "                future_t_len = [executor.submit(get_words_counter,i) for i in range(len(str_lists))]\n",
    "                with tqdm(desc=str(cnt),total=len(future_t_len)) as pbar:\n",
    "                    for future in concurrent.futures.as_completed(future_t_len):\n",
    "                        new_counter.update(future.result())\n",
    "                        pbar.update(1)\n",
    "\n",
    "#收尾工作，剩余数据可能凑不齐一个批次\n",
    "str_lists = [str_list[i*batch:(i+1)*batch] for i in range(len(str_list)//batch + (len(str_list)%batch != 0))]\n",
    "str_list = []\n",
    "str_len = 0\n",
    "cnt += 1\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=15) as executor:\n",
    "    future_t_len = [executor.submit(get_words_counter,i) for i in range(len(str_lists))]\n",
    "    with tqdm(desc=str(cnt),total=len(future_t_len)) as pbar:\n",
    "        for future in concurrent.futures.as_completed(future_t_len):\n",
    "            new_counter.update(future.result())\n",
    "            pbar.update(1)\n",
    "\n",
    "#整理词表\n",
    "dict_to_write = dict()\n",
    "for k,v in new_counter.items():\n",
    "    try:\n",
    "        k = k.decode()\n",
    "        if k != '\\r' and k != '\\n':\n",
    "            dict_to_write[k] = v\n",
    "    except:\n",
    "        pass\n",
    "#写入文件\n",
    "with open(\"vocabs/vocab_b_\"+str(len(dict_to_write))+\".txt\",\"w\",encoding='utf-8') as f:\n",
    "    for k,v in dict_to_write.items():\n",
    "        print(k+'\\t'+str(v),file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
