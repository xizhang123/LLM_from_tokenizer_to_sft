训练时，训练程序在后台运行，用户可以在ipynb文件中随时监控训练状态，通过执行特殊函数或修改全局变量对训练过程进行调整，十分灵活（但只对写程序的人本身友好）
特殊函数说明：
TOGGLE()：[暂停/继续]训练状态切换，暂停后检查显卡占用，确保训练真的暂停后，可以安全的修改全局变量，调整训练过程。
暂停后建议现保存模型权重，如果全局变量设置不当，造成效果退化/显存溢出等问题可以补救。
STOP()：彻底终止训练，无法通过TOGGLE()恢复。
SET_CACULATE_SIZE()：设置实际计算时的批次，当前方法使用梯度累加来模拟大batch_size，用这个方法避免显存爆炸。
seq_len：全局变量，设置当前语料怎么切分，越短，上下文信息少，但快速，越长，上下文越丰富，但更慢。
batch_size_adjust：调整批次大小，batch_size 默认为 1024，batch_size_adjust默认为0。batch_size_adjust = 128 - 1024 是设置批次大小为128的意思。
optimizer.set_lrate(1e-5,8192)：设置学习率，以及将学习率过渡到新的值需要经历多少步（二次函数平滑过渡，不是线性过渡）。
optimizer.enable_wave = False：是否使用余弦模拟退火（连续版本，不是突然降温的版本）
get_info：设置那些参数应当视为矩阵，那些应当视为向量，以便使用Muon优化器，如果模型结构有调整，需要修改这个函数内部的信息，确保正确。
其余说明：
模型支持一些更加精细的设置，.py文件中也有一些功能没有使用，因此发现[无用/错误/未实现/未知]的功能不必困惑，精力有限，这里只确保训练代码能正确运行，没有剔除无关代码。
修正之前，没有有实现错误，使用的是Adam，有少量损失尖刺，发现后及时进行了排查，于是有了修正版本。考虑到训练成本，修正版本是接着Adam版本继续训练的。
