EL-Attention参考
Yan Y, Chen J, Qi W, et al. El-attention: Memory efficient lossless attention for generation[C]//International Conference on Machine Learning. PMLR, 2021: 11648-11658.
ALiBi位置编码参考
Press O, Smith N A, Lewis M. Train short, test long: Attention with linear biases enables input length extrapolation[J]. arXiv preprint arXiv:2108.12409, 2021.
这里的推理优化是指实现了ALiBi（魔改版本）与形式EL-Attention版本KV-Cache的融合。
需要注意的是，训练代码与推理加速代码的逻辑是不同的。训练时，不使用EL-Attention，而推理是用于节省缓存。
原因是，EL-Attention不会节省训练资源，智能在推理时无损但节省内存。
值得一提的是（参考https://spaces.ac.cn/archives/10091）DeepSeek的MLA是将RoPE与类似技术进行了结合。
因此，可以不要脸的说，这里的优化，也是一种程度外推编码与显存节省策略的融合，MLA是异曲同工的。
优化策略通过各方“参考”拼凑得到的，部分思想的来源已经不可考，但主要源自“苏神的博客”和“EL-Attention作者的报告会”。
由于时间仓促，虽然这件事是真的做成了，但代码的可读性很低，暂时以半成品开源，后续有时间继续优化。
